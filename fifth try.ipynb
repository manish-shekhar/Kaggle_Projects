{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf550ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manish/opt/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from ipywidgets import interact\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder,LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from fancyimpute import KNN, IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7758a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rstr(df):\n",
    "    counts = df.apply(lambda x: x.count()).values\n",
    "    uniques = df.apply(lambda x: x.unique()).values\n",
    "    unique_count = [len(uniques[i]) for i in range(len(uniques))]\n",
    "    na_val = df.apply(lambda x: x.isna().sum()).values\n",
    "    na_perc = na_val/len(df)\n",
    "    data_type = df.dtypes\n",
    "    mean = df.apply(lambda x:x.mean() if x.dtypes in ['int64','float64'] else 'NaN')\n",
    "    std_dev = df.apply(lambda x:np.std(x) if x.dtypes in ['int64','float64'] else 'NaN')\n",
    "    quantile_25 = df.apply(lambda x: x.quantile(0.25) if x.dtypes in ['int64','float64'] else 'NaN')\n",
    "    quantile_75 = df.apply(lambda x: x.quantile(0.75) if x.dtypes in ['int64','float64'] else 'NaN')\n",
    "    min_val = df.apply(lambda x: x.min() if x.dtypes in ['int64','float64'] else 'NaN')\n",
    "    max_val = df.apply(lambda x: x.max() if x.dtypes in ['int64','float64'] else 'NaN')\n",
    "    d = pd.DataFrame({'counts': counts,'uniques':uniques,\n",
    "                      'na_val':na_val,'Na_perc':na_perc,'data_type':data_type,\n",
    "                      'mean':mean,'std_dev':std_dev,'Q25': quantile_25,'Q75':quantile_75,\n",
    "                      'min':min_val,'max':max_val,'uniques_counts':unique_count},\n",
    "                     index = df.columns)\n",
    "    return d\n",
    "\n",
    "def ohe(df,column):\n",
    "    df=df.copy()\n",
    "    dummies = pd.get_dummies(df[column],prefix=column)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4c7746",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n",
      "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
      "       'Transported', 'NumGroup', 'Group', 'Deck', 'Side', 'is_alone',\n",
      "       'tot_exp', 'age_bins', 'RoomService_bins', 'FoodCourt_bins',\n",
      "       'ShoppingMall_bins', 'Spa_bins', 'VRDeck_bins', 'exp_bins', 'exp_flag',\n",
      "       'RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag',\n",
      "       'VRDeck_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "df1 = train_df.copy()\n",
    "df2 = test_df.copy()\n",
    "\n",
    "combine = pd.concat([df1, df2], axis=0, join=\"outer\")\n",
    "\n",
    "#Deck and side as separate columns\n",
    "\n",
    "combine[\"NumGroup\"] = combine[\"PassengerId\"].apply(lambda x: x.rsplit('_')[1])\n",
    "combine[\"Group\"] = combine[\"PassengerId\"].apply(lambda x: x.rsplit('_')[0])\n",
    "combine[\"Deck\"] = combine[\"Cabin\"].apply(lambda x: None if (str(x)==\"\" or str(x)=='nan') else str(x).rsplit('/')[0])\n",
    "combine[\"Side\"] = combine[\"Cabin\"].apply(lambda x: None if (str(x)==\"\" or str(x)=='nan') else str(x).rsplit('/')[2])\n",
    "\n",
    "combine.NumGroup = combine.NumGroup.astype(int)\n",
    "combine.Group = combine.Group.astype(int)\n",
    "\n",
    "# Convert ordinal to numeric Deck and side\n",
    "\n",
    "combine.Deck=combine.Deck.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'T':7})\n",
    "\n",
    "combine.Side=combine.Side.replace({'P':0,'S':1})\n",
    "\n",
    "# encode ordinal variables\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "for col in ['HomePlanet','CryoSleep','VIP','Destination','Transported','Deck','Side','NumGroup']:\n",
    "    \n",
    "    combine.loc[:,col] = encoder.fit_transform(combine.loc[:,col].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "combine['is_alone'] = combine['NumGroup'].apply(lambda x: 1 if x==0 else 0).astype(int)\n",
    "    \n",
    "# drop name column\n",
    "\n",
    "combine.drop('Name',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# using Iterative Imputer fancyimpute to fill na data in categorical independent variables\n",
    "\n",
    "imp_num = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10,max_features=3,min_samples_leaf=4),\n",
    "                               initial_strategy='mean',\n",
    "                               max_iter=10, random_state=0)\n",
    "imp_cat = IterativeImputer(estimator=RandomForestClassifier(n_estimators=10,max_features=3,min_samples_leaf=4), \n",
    "                               initial_strategy='most_frequent',\n",
    "                               max_iter=10, random_state=0)\n",
    "\n",
    "all_cols = ['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Deck', 'Side','Age',\n",
    "            'RoomService', 'FoodCourt', 'ShoppingMall',\n",
    "            'Spa', 'VRDeck']\n",
    "\n",
    "cat_cols = ['HomePlanet', 'CryoSleep', 'Destination','VIP', \n",
    "          'Deck', 'Side']\n",
    "\n",
    "num_cols = ['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Missing values addition through Random forest\n",
    "\n",
    "comb = combine.loc[:,all_cols].copy()\n",
    "comb2 = comb.copy()\n",
    "\n",
    "comb.loc[:,cat_cols] = SimpleImputer(strategy = 'most_frequent').fit_transform(comb.loc[:,cat_cols])\n",
    "\n",
    "comb.loc[:,num_cols] = SimpleImputer(strategy = 'mean').fit_transform(comb.loc[:,num_cols])\n",
    "\n",
    "for col in cat_cols:\n",
    "    df2 = comb.copy()\n",
    "    df2.loc[:,col] = comb2.loc[:,col]\n",
    "    X_tr = df2[df2.loc[:,col].notna()].drop([col],axis=1).values\n",
    "    \n",
    "    X_te = df2[df2.loc[:,col].isna()].drop([col],axis=1).values\n",
    "    y_tr = df2[df2.loc[:,col].notna()].loc[:,col].values\n",
    "    modl = RandomForestClassifier()\n",
    "    modl.fit(X_tr,y_tr)\n",
    "    predicted = modl.predict(X_te)\n",
    "    df2.loc[df2.loc[:,col].isna(),col] = predicted\n",
    "    comb.loc[:,col] = df2.loc[:,col]\n",
    "    \n",
    "\n",
    "for col in num_cols:\n",
    "    df2 = comb.copy()\n",
    "    df2.loc[:,col] = comb2.loc[:,col]\n",
    "    X_tr = df2[df2.loc[:,col].notna()].drop([col],axis=1).values\n",
    "    \n",
    "    X_te = df2[df2.loc[:,col].isna()].drop([col],axis=1).values\n",
    "    y_tr = df2[df2.loc[:,col].notna()].loc[:,col].values\n",
    "    modl = RandomForestRegressor()\n",
    "    modl.fit(X_tr,y_tr)\n",
    "    predicted = modl.predict(X_te)\n",
    "    df2.loc[df2.loc[:,col].isna(),col] = predicted\n",
    "    comb.loc[:,col] = df2.loc[:,col]\n",
    "\n",
    "combine.loc[:,all_cols] = comb\n",
    "# def num_imputer(num_col):\n",
    "    \n",
    "#     df2 = comb.loc[:,all_cols].copy()\n",
    "#     df2.loc[:,num_col] = comb2.loc[:,num_col]\n",
    "#     return pd.DataFrame(imp_num.fit_transform(df2),columns = df2.columns).loc[:,num_col]\n",
    "\n",
    "# def cat_imputer(cat_col):\n",
    "    \n",
    "#     df2 = comb.loc[:,all_cols].copy()\n",
    "#     df2.loc[:,cat_col] = comb2.loc[:,cat_col]\n",
    "# #     df2.loc[:,cat_col] = df2.loc[:,cat_col].fillna(-10)\n",
    "    \n",
    "#     print(cat_col,rstr(df2))\n",
    "#     return pd.DataFrame(imp_cat.fit_transform(df2),columns = df2.columns).loc[:,cat_col]\n",
    "\n",
    "\n",
    " # for num_col in num_cols:\n",
    " #     combine.loc[:,num_col] = num_imputer(num_col)\n",
    "\n",
    "# for cat_col in cat_cols:\n",
    "#     combine.loc[:,cat_col] = cat_imputer(cat_col)\n",
    "\n",
    "\n",
    "# total expense column\n",
    "\n",
    "combine['tot_exp'] = combine.loc[:,['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1,skipna=False)\n",
    "\n",
    "# # make buckets\n",
    "\n",
    "def make_bins(num_bins,df,col,bin_col_name):\n",
    "    exp_quantile = df.loc[:,col].quantile(np.linspace(0,1,num_bins+1))\n",
    "    bins = exp_quantile.values\n",
    "\n",
    "    df[bin_col_name] = None\n",
    "    ff=0\n",
    "    gg=-1\n",
    "\n",
    "    for i in bins[1:]:\n",
    "        mask = ((df.loc[:,col] <=i) & (df.loc[:,col] > gg))\n",
    "        df.loc[mask,bin_col_name] = ff\n",
    "        ff+=1\n",
    "        gg=i\n",
    "\n",
    "yu=0\n",
    "col_names = ['age_bins','RoomService_bins', 'FoodCourt_bins', 'ShoppingMall_bins', 'Spa_bins', 'VRDeck_bins','exp_bins']\n",
    "for col in ['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp']:\n",
    "    make_bins(4,combine,col,col_names[yu])\n",
    "    yu+=1\n",
    "\n",
    "combine['exp_flag'] = combine['exp_bins'].apply(lambda x: 0 if x==0 else 1).astype(int)\n",
    "combine['RoomService_flag'] = combine['RoomService_bins'].apply(lambda x: 0 if x==0 else 1).astype(int)\n",
    "combine['FoodCourt_flag'] = combine['FoodCourt_bins'].apply(lambda x: 0 if x==0 else 1).astype(int)\n",
    "combine['ShoppingMall_flag'] = combine['ShoppingMall_bins'].apply(lambda x: 0 if x==0 else 1).astype(int)\n",
    "combine['Spa_flag'] = combine['Spa_bins'].apply(lambda x: 0 if x==0 else 1).astype(int)\n",
    "combine['VRDeck_flag'] = combine['VRDeck_bins'].apply(lambda x: 0 if x==0 else 1).astype(int)\n",
    "\n",
    "\n",
    "# convert bins as int\n",
    "cols = ['age_bins','RoomService_bins', 'FoodCourt_bins', 'ShoppingMall_bins', 'Spa_bins', 'VRDeck_bins','exp_bins']\n",
    "\n",
    "for col in cols:\n",
    "    combine.loc[:,col] = combine.loc[:,col].apply(lambda x: None if (x==\"\" or x==None or x=='nan') else int(x))\n",
    "\n",
    "\n",
    "# reset index of dataset\n",
    "\n",
    "combine.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# scale variables\n",
    "\n",
    "scaler = MinMaxScaler(copy=False)\n",
    "\n",
    "cols = ['HomePlanet', 'CryoSleep',  'Destination', 'Age',\n",
    "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
    "       'Transported', 'NumGroup', 'Group', 'Deck', 'Side', 'is_alone',\n",
    "       'tot_exp', 'age_bins', 'RoomService_bins', 'FoodCourt_bins',\n",
    "       'ShoppingMall_bins', 'Spa_bins', 'VRDeck_bins', 'exp_bins', 'exp_flag',\n",
    "       'RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag',\n",
    "       'VRDeck_flag']\n",
    "\n",
    "for col in cols:\n",
    "    k = combine.loc[:,col].values.reshape(-1,1)\n",
    "    scaler.fit(k)\n",
    "    combine.loc[:,col]=np.around(scaler.transform(k),3)\n",
    "\n",
    "print(combine.columns)\n",
    "\n",
    "\n",
    "# # # Visualization\n",
    "\n",
    "# # g = sns.FacetGrid(combine, col = 'CryoSleep')\n",
    "# # p1 = g.map(sns.histplot,'HomePlanet')\n",
    "# rstr(combine)\n",
    "\n",
    "# rs = np.random.RandomState(0)\n",
    "# df = pd.DataFrame(rs.rand(1, 1))\n",
    "# corr = combine.corr()\n",
    "# corr.style.background_gradient(cmap='bwr')\n",
    "# corr\n",
    "# corr['CryoSleep'].nlargest(n=10)\n",
    "\n",
    "\n",
    "# def plot_feature(bin_number,col='RoomService'):\n",
    "#     maximum = max(combine[col])\n",
    "#     divider = int(maximum/bin_number)\n",
    "#     df_temp2 = combine.copy()\n",
    "#     df_temp2[col] = (df_temp2[col]//divider)\n",
    "#     plt.figure(figsize=(16,4))\n",
    "#     sns.histplot(data=df_temp2, x=col, hue='Transported', binwidth=1)\n",
    "#     #sns.countplot(data=df_temp2, x='GroupId', hue='Transported') \n",
    "#     plt.title('gggg distribution')\n",
    "#     plt.xticks(np.linspace(0,maximum//divider,bin_number+1),\n",
    "#                np.linspace(0,maximum,bin_number+1).astype(int),rotation=45)\n",
    "#     plt.show()\n",
    "\n",
    "# interact(plot_feature, \n",
    "#          bin_number = (0,20,1),\n",
    "#         )\n",
    "# rstr(combine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010bf517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'CryoSleep', 'Cabin', 'Age', 'VIP', 'RoomService',\n",
       "       'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'Group',\n",
       "       'tot_exp', 'age_bins', 'RoomService_bins', 'FoodCourt_bins',\n",
       "       'ShoppingMall_bins', 'Spa_bins', 'VRDeck_bins', 'exp_bins', 'exp_flag',\n",
       "       'RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag',\n",
       "       'VRDeck_flag', 'HomePlanet_0.0', 'HomePlanet_0.5', 'HomePlanet_1.0',\n",
       "       'Destination_0.0', 'Destination_0.5', 'Destination_1.0', 'NumGroup_0.0',\n",
       "       'NumGroup_0.143', 'NumGroup_0.286', 'NumGroup_0.429', 'NumGroup_0.571',\n",
       "       'NumGroup_0.714', 'NumGroup_0.857', 'NumGroup_1.0', 'Deck_0.0',\n",
       "       'Deck_0.143', 'Deck_0.286', 'Deck_0.429', 'Deck_0.571', 'Deck_0.714',\n",
       "       'Deck_0.857', 'Deck_1.0', 'Side_0.0', 'Side_1.0', 'is_alone_0.0',\n",
       "       'is_alone_1.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for col in ['HomePlanet','Destination','NumGroup','Deck', 'Side', 'is_alone']:\n",
    "#     combine = ohe(combine,col)\n",
    "\n",
    "\n",
    "\n",
    "rstr(combine)\n",
    "\n",
    "# scale variables\n",
    "\n",
    "# scaler = MinMaxScaler(copy=False)\n",
    "\n",
    "# cols = ['HomePlanet', 'CryoSleep',  'Destination', 'Age',\n",
    "#        'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
    "#        'Transported', 'NumGroup', 'Group', 'Deck', 'Side', 'is_alone',\n",
    "#        'tot_exp', 'age_bins', 'RoomService_bins', 'FoodCourt_bins',\n",
    "#        'ShoppingMall_bins', 'Spa_bins', 'VRDeck_bins', 'exp_bins', 'exp_flag',\n",
    "#        'RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag',\n",
    "#        'VRDeck_flag']\n",
    "\n",
    "# for col in cols:\n",
    "#     k = combine.loc[:,col].values.reshape(-1,1)\n",
    "#     scaler.fit(k)\n",
    "#     combine.loc[:,col]=np.around(scaler.transform(k),3)\n",
    "\n",
    "# def plot_feature(bin_number,col='VIP'):\n",
    "#     maximum = max(combine[col])\n",
    "#     divider = int(maximum/bin_number)\n",
    "#     df_temp2 = combine.copy()\n",
    "#     df_temp2[col] = (df_temp2[col]//divider)\n",
    "#     plt.figure(figsize=(16,4))\n",
    "#     sns.histplot(data=df_temp2, x=col, hue='Transported', binwidth=1)\n",
    "#     #sns.countplot(data=df_temp2, x='GroupId', hue='Transported') \n",
    "#     plt.title('gggg distribution')\n",
    "#     plt.xticks(np.linspace(0,maximum//divider,bin_number+1),\n",
    "#                np.linspace(0,maximum,bin_number+1).astype(int),rotation=45)\n",
    "#     plt.show()\n",
    "\n",
    "# interact(plot_feature, \n",
    "#          bin_number = (1,2,1),\n",
    "#         )\n",
    "\n",
    "\n",
    "combine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b670d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7823, 44) (870, 44) (7823,) (870,)\n",
      "\n",
      " LGBMClassifier(colsample_bytree=1, learning_rate=0.4, n_estimators=80,\n",
      "               num_leaves=50, subsample=0.6) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.80       438\n",
      "         1.0       0.80      0.81      0.80       432\n",
      "\n",
      "    accuracy                           0.80       870\n",
      "   macro avg       0.80      0.80      0.80       870\n",
      "weighted avg       0.80      0.80      0.80       870\n",
      " [[349  89]\n",
      " [ 83 349]]\n",
      "Accuracy:  0.8022988505747126\n"
     ]
    }
   ],
   "source": [
    "# split modified combine to train and test file dfs as provided in competition\n",
    "\n",
    "train_df = combine.loc[combine.loc[:,'Transported'].notna()]\n",
    "test_df = combine.loc[combine.loc[:,'Transported'].isna()]\n",
    "\n",
    "# select feature variables. We will reduce variables here for respective models after feature selection of that model\n",
    "# to include relevant features\n",
    "\n",
    "X_train_df = train_df.loc[:,['CryoSleep', 'VIP', 'Age', 'Group','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp','exp_bins','age_bins','exp_flag','RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag','VRDeck_flag','HomePlanet_0.0', 'HomePlanet_0.5', 'HomePlanet_1.0',\n",
    "       'Destination_0.0', 'Destination_0.5', 'Destination_1.0', 'NumGroup_0.0',\n",
    "       'NumGroup_0.143', 'NumGroup_0.286', 'NumGroup_0.429', 'NumGroup_0.571',\n",
    "       'NumGroup_0.714', 'NumGroup_0.857', 'NumGroup_1.0', 'Deck_0.0',\n",
    "       'Deck_0.143', 'Deck_0.286', 'Deck_0.429', 'Deck_0.571', 'Deck_0.714',\n",
    "       'Deck_0.857', 'Deck_1.0', 'Side_0.0', 'Side_1.0', 'is_alone_0.0',\n",
    "       'is_alone_1.0']]\n",
    "X_test_df = test_df.loc[:,['CryoSleep', 'VIP', 'Age', 'Group','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp','exp_bins','age_bins','exp_flag','RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag','VRDeck_flag','HomePlanet_0.0', 'HomePlanet_0.5', 'HomePlanet_1.0',\n",
    "       'Destination_0.0', 'Destination_0.5', 'Destination_1.0', 'NumGroup_0.0',\n",
    "       'NumGroup_0.143', 'NumGroup_0.286', 'NumGroup_0.429', 'NumGroup_0.571',\n",
    "       'NumGroup_0.714', 'NumGroup_0.857', 'NumGroup_1.0', 'Deck_0.0',\n",
    "       'Deck_0.143', 'Deck_0.286', 'Deck_0.429', 'Deck_0.571', 'Deck_0.714',\n",
    "       'Deck_0.857', 'Deck_1.0', 'Side_0.0', 'Side_1.0', 'is_alone_0.0',\n",
    "       'is_alone_1.0']]\n",
    "\n",
    "# X_train_df = train_df.loc[:,['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp']]\n",
    "# X_test_df = test_df.loc[:,['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp']]\n",
    "\n",
    "\n",
    "y_train_df = train_df.loc[:,['Transported']]\n",
    "\n",
    "# to test on train.csv, splitting into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_df.values, y_train_df.values, \n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "y_train = y_train[:,0]\n",
    "y_test = y_test[:,0]\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "\n",
    "# start modelling. Checking performance on each model after feature selection and cross validation. Uncomment whichever\n",
    "# model to train and validate\n",
    "\n",
    "\n",
    "# model = LogisticRegression(solver='saga',max_iter = 500)\n",
    "\n",
    "# # feature selection\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# feature_scores = pd.Series(model.coef_[0], index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "#gridsearch cv for cross validation\n",
    "\n",
    "# param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "#               'class_weight': [None,'balanced'], \n",
    "#               'penalty': ['l2','l1','elasticnet']}\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, refit = True, verbose = 3,n_jobs=-1,cv=10) \n",
    "\n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) \n",
    " \n",
    "# # print best parameter after tuning \n",
    "\n",
    "# print(grid.best_params_) \n",
    "\n",
    "\n",
    "# model = RandomForestClassifier(criterion= 'entropy', max_features= 4, max_samples= 250, min_samples_split= 5, n_estimators= 20)\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "# # feature selection\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# feature_scores = pd.Series(model.feature_importances_, index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "# # gridsearch cv for cross validation\n",
    "\n",
    "# param_grid = {'n_estimators': [5,10,15,20],  \n",
    "#               'criterion': ['gini', 'entropy'], \n",
    "#               'min_samples_split': [2,5,10],\n",
    "#                'max_features': [1,2,3,4],\n",
    "#                 'max_samples': [50,100,150,200,250]}\n",
    "\n",
    "\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, refit = True, verbose = 0,n_jobs=-1,cv=10) \n",
    "\n",
    "# # fitting the model for grid search \n",
    "\n",
    "# grid.fit(X_train, y_train) \n",
    " \n",
    "# # print best parameter after tuning \n",
    "\n",
    "# print(grid.best_params_) \n",
    "\n",
    "\n",
    "# model = AdaBoostClassifier()\n",
    "# model = AdaBoostClassifier(base_estimator=None, random_state=42,algorithm='SAMME.R',n_estimators = 50,learning_rate=0.6)\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# feature_scores = pd.Series(model.feature_importances_, index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "# param_grid = {'learning_rate': [0.2,0.4,0.6,0.8,1],  \n",
    "#               'n_estimators': [5,10,20,30,40,50]}\n",
    "\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, refit = True, verbose = 0,n_jobs=-1,cv=10) \n",
    "\n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) \n",
    " \n",
    "# # print best parameter after tuning \n",
    "\n",
    "# print(grid.best_params_) \n",
    "\n",
    "\n",
    "\n",
    "# model = GradientBoostingClassifier(random_state=42,learning_rate= 0.2, max_features= 3, n_estimators= 300, subsample= 0.8)\n",
    "\n",
    "# model = GradientBoostingClassifier()\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# feature_scores = pd.Series(model.feature_importances_, index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "# param_grid = {'learning_rate': [0.2,0.4,0.6,0.8,1],  \n",
    "#               'n_estimators': [5,10,20,30,40,50],\n",
    "#              'subsample': [0.2,0.4,0.6,0.8,1],\n",
    "#              'max_features': [2,3,4,5,6]}\n",
    "\n",
    "\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, refit = True, verbose = 0,n_jobs=-1,cv=5) \n",
    "\n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) \n",
    " \n",
    "# # print best parameter after tuning \n",
    "\n",
    "# print(grid.best_params_) \n",
    "\n",
    "\n",
    "\n",
    "# model = XGBClassifier(use_label_encoder=False,objective= 'binary:logistic', learning_rate= 0.4, \n",
    "#                       max_depth= 8,alpha= 10, n_estimators= 500, subsample= 0.8, colsample_bytree= 0.5, \n",
    "#                       colsample_bylevel= 0.7, colsample_bynode= 1)\n",
    "# model = XGBClassifier(silent=True)\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "\n",
    "# feature_scores = pd.Series(model.feature_importances_, index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "\n",
    "# data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n",
    "# param_list = {\"objective\":['binary:logistic','reg:squarederror','reg:logistic','binary:logitraw','binary:hinge'],\n",
    "#               'learning_rate': [0.3,0.7,1,2,3],\n",
    "#                 'max_depth': [3,5,6,8,10], 'alpha': [10,13,16,20,25],\n",
    "#               'n_estimators': [50,100,150,200],'subsample' : [0.2,0.3,0.6,0.8,1],\n",
    "#              'colsample_bytree': [0.3,0.5,0.1,0.7,1], 'colsample_bylevel' :[0.3,0.5,0.1,0.7,1],\n",
    "#              'colsample_bynode': [0.3,0.5,0.1,0.7,1]}\n",
    "\n",
    "# cv_result = pd.DataFrame(index = pd.Series(range(20)).values,columns = ['params','score'])\n",
    "\n",
    "# cv_result.loc[:,'params'] = str()\n",
    "# cv_result.loc[:,'score'] = float()\n",
    "\n",
    "# for i in range(100):\n",
    "#     params = dict()\n",
    "\n",
    "#     for key in param_list.keys():        \n",
    "#         params[key] = random.choice(param_list[key])\n",
    "\n",
    "#     xgb_cv = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n",
    "#                         num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "#     cv_result.loc[i,['params']] = str(params)\n",
    "#     cv_result.loc[i,['score']] = xgb_cv.iloc[-1,0]\n",
    "\n",
    "# cv_result.loc[cv_result.loc[:,'score'].idxmax()]['params'],cv_result.loc[cv_result.loc[:,'score'].idxmax()]['score']\n",
    "    \n",
    "# model = CatBoostClassifier(depth=5,iterations=500,learning_rate=0.2,silent=True)\n",
    "\n",
    "# model = CatBoostClassifier(silent=True)\n",
    "\n",
    "# feature selection\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# feature_scores = pd.Series(model.feature_importances_, index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "# gridsearch cv for cross validation\n",
    "\n",
    "# param_grid = {'depth'         : [4,5,6,7,8,9, 10],\n",
    "#               'learning_rate' : [0.1,0.3,0.5,0.7,1],\n",
    "#               'iterations'    : [10, 20,30,40,50,60,70]\n",
    "#              }\n",
    "\n",
    "\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, refit = True, verbose = 0,n_jobs=-1,cv=10) \n",
    "\n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) \n",
    " \n",
    "# # print best parameter after tuning \n",
    "\n",
    "# print(grid.best_params_) \n",
    "\n",
    "\n",
    "model = LGBMClassifier(boosting_type='gbdt',colsample_bytree=1,learning_rate=0.4,n_estimators=80,num_leaves=50,subsample=0.6)\n",
    "\n",
    "# model = LGBMClassifier()\n",
    "\n",
    "# # feature selection\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# feature_scores = pd.Series(model.feature_importances_, index=X_train_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# feature_scores\n",
    "\n",
    "# gridsearch cv for cross validation\n",
    "\n",
    "# param_grid = {'boosting_type' : ['gbdt','goss','rf'],\n",
    "#               'num_leaves' : [40,50],\n",
    "#               'learning_rate'    : [0.4,0.6],\n",
    "#               'n_estimators' : [60,80,100],\n",
    "#               'subsample' : [0.6,0.8,1],\n",
    "#               'colsample_bytree' : [0.6,0.8,1]\n",
    "#              }\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, refit = True, verbose = 0,n_jobs=-1,cv=5) \n",
    "\n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) \n",
    " \n",
    "# # print best parameter after tuning \n",
    "\n",
    "# print(grid.best_params_) \n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "cm=confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"\\n\",model,\"\\n\",\n",
    "          classification_report(y_test, pred),cm)\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "print(\"Accuracy: \",accuracy_score(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "# X_train1 = X_train_df.values\n",
    "# y_train1 = y_train_df\n",
    "# X_test1 = X_test_df.values\n",
    "\n",
    "# model.fit(X_train1,y_train1)\n",
    "# pred = model.predict(X_test1)\n",
    "# print(pred.shape, test_df.shape)\n",
    "# submit_df = pd.DataFrame({'PassengerId': pd.Series(dtype='str'),\n",
    "#                    'Transported': pd.Series(dtype='int')})\n",
    "# submit_df.loc[:,'PassengerId'] = test_df.loc[:,'PassengerId']\n",
    "# submit_df.loc[:,'Transported'] = pred\n",
    "# submit_df.loc[:,'Transported'] = submit_df.loc[:,'Transported'].astype('boolean')\n",
    "\n",
    "# submit_df\n",
    "# submit_df.to_csv('titanic_spaceship_submit6.csv',index=False)\n",
    "\n",
    "\n",
    "# combining model results together\n",
    "\n",
    "# models = [GradientBoostingClassifier(random_state=42,learning_rate= 0.4, max_features= 5, n_estimators= 50, subsample= 1),\n",
    "#          XGBClassifier(use_label_encoder=False,objective= 'reg:logistic',learning_rate= 0.3,max_depth= 8,alpha= 10, n_estimators= 50, subsample= 0.6,colsample_bytree= 1, colsample_bylevel= 0.7, colsample_bynode= 0.7),\n",
    "#          CatBoostClassifier(depth=5,iterations=900,learning_rate=0.1,silent=True),\n",
    "#          LGBMClassifier(boosting_type='gbdt',colsample_bytree=1,learning_rate=0.2,n_estimators=80,num_leaves=40,subsample=0.6)]\n",
    "\n",
    "\n",
    "\n",
    "# models_features = [['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp'],\n",
    "#                   ['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp'],\n",
    "#                   ['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp'],\n",
    "#                   ['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp']]\n",
    "\n",
    "# models = [CatBoostClassifier(depth=5,iterations=800,learning_rate=0.1,silent=True),\n",
    "#          LGBMClassifier(boosting_type='gbdt',colsample_bytree=1,learning_rate=0.2,n_estimators=100,num_leaves=40,subsample=0.6)]\n",
    "\n",
    "\n",
    "\n",
    "# models_features = [['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp','exp_bins','is_alone','age_bins','exp_flag','RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag','VRDeck_flag'],\n",
    "#                   ['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','Group','NumGroup','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp','exp_bins','is_alone','age_bins','exp_flag','RoomService_flag', 'FoodCourt_flag', 'ShoppingMall_flag', 'Spa_flag','VRDeck_flag']]\n",
    "\n",
    "\n",
    "# # weights = pd.Series([1,1,5,5])\n",
    "# wt_lst = [pd.Series([1,1])]\n",
    "# # ,pd.Series([1,1,2,2]),pd.Series([1,1,3,3])]\n",
    "\n",
    "# for weights in wt_lst:\n",
    "\n",
    "#     o=0\n",
    "#     predictions = pd.DataFrame()\n",
    "    \n",
    "#     for model in models:\n",
    "#         X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train_df.loc[:,models_features[o]].values, y_train_df.values, \n",
    "#                                                         test_size=0.1, random_state=0)\n",
    "#         y_train1 = y_train1[:,0]\n",
    "#         y_test1 = y_test1[:,0]\n",
    "#         folds = StratifiedKFold(shuffle=True,n_splits=10)\n",
    "\n",
    "#         y_probs = list()\n",
    "\n",
    "#         for fold, (train_id, test_id) in enumerate(folds.split(X_train1, y_train1)):\n",
    "            \n",
    "#             # Split data\n",
    "#             X_train2 = X_train1[train_id]\n",
    "#             y_train2 = y_train1[train_id]\n",
    "#             X_valid2 = X_train1[test_id]\n",
    "#             y_valid2 = y_train1[test_id]\n",
    "\n",
    "#             model.fit(X_train2, y_train2)\n",
    "\n",
    "#             # test\n",
    "#             y_probs.append(model.predict_proba(X_test1)[:,1])\n",
    "# #             y_probs.append(model.predict(X_test1))\n",
    "            \n",
    "#         model_probs = pd.DataFrame(y_probs).T\n",
    "#         model_probs.loc[:,\"Mean\"] = model_probs.mean(axis=1)\n",
    "        \n",
    "#         predictions.loc[:,o] = model_probs.loc[:,'Mean'].values\n",
    "        \n",
    "#         o=o+1\n",
    "        \n",
    "\n",
    "#     predictions.loc[:,'Total'] = predictions.dot(weights)\n",
    "\n",
    "#     predictions.loc[:,'perc_true'] = predictions.loc[:,'Total']/sum(weights)\n",
    "#     predictions.loc[:,'Final_pred'] = predictions.loc[:,'perc_true'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "\n",
    "#     final_pred = predictions.loc[:,'Final_pred']\n",
    "\n",
    "\n",
    "#     cm=confusion_matrix(y_test1, final_pred)\n",
    "\n",
    "#     print(\"\\n\",\"Ensemble\",\"\\n\",weights,\n",
    "#               classification_report(y_test1, final_pred),cm)\n",
    "#     print(\"Accuracy score:\",accuracy_score(y_test1, final_pred))\n",
    "\n",
    "\n",
    "\n",
    "# # training ensemble on full train.csv\n",
    "\n",
    "# models = [CatBoostClassifier(depth=5,iterations=800,learning_rate=0.1,silent=True),\n",
    "#          LGBMClassifier(boosting_type='gbdt',colsample_bytree=1,learning_rate=0.2,n_estimators=100,num_leaves=40,subsample=0.6)]\n",
    "\n",
    "\n",
    "\n",
    "# models_features = [['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','NumGroup','Group','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp','is_alone','exp_bins','age_bins'],\n",
    "#                   ['HomePlanet', 'CryoSleep', 'Destination','VIP', 'Age', 'Deck', 'Side','NumGroup','Group','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','tot_exp','is_alone','exp_bins','age_bins']]\n",
    "\n",
    "# weights = pd.Series([1,1])\n",
    "\n",
    "# o=0\n",
    "# predictions = pd.DataFrame()\n",
    "# for model in models:\n",
    "#     X_train1 = X_train_df.loc[:,models_features[o]].values\n",
    "#     y_train1 = y_train_df.values[:,0]\n",
    "#     X_test1 = X_test_df.loc[:,models_features[o]].values\n",
    "    \n",
    "#     folds = StratifiedKFold(shuffle=True,n_splits=10)\n",
    "\n",
    "#     y_probs = list()\n",
    "\n",
    "#     for fold, (train_id, test_id) in enumerate(folds.split(X_train1, y_train1)):\n",
    "\n",
    "#         # Split data\n",
    "#         X_train2 = X_train1[train_id]\n",
    "#         y_train2 = y_train1[train_id]\n",
    "#         X_valid2 = X_train1[test_id]\n",
    "#         y_valid2 = y_train1[test_id]\n",
    "\n",
    "#         model.fit(X_train2, y_train2)\n",
    "\n",
    "#         # test\n",
    "# #         y_probs.append(model.predict_proba(X_test1)[:,1])\n",
    "#         y_probs.append(model.predict(X_test1))\n",
    "\n",
    "#     model_probs = pd.DataFrame(y_probs).T\n",
    "#     model_probs.loc[:,\"Mean\"] = model_probs.mean(axis=1)\n",
    "        \n",
    "#     predictions.loc[:,o] = model_probs.loc[:,'Mean'].values\n",
    "#     o=o+1\n",
    "\n",
    "\n",
    "# predictions.loc[:,'Total'] = predictions.dot(weights)\n",
    "# predictions.loc[:,'perc_true'] = predictions.loc[:,'Total']/sum(weights)\n",
    "# predictions.loc[:,'Final_pred'] = predictions.loc[:,'perc_true'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "\n",
    "\n",
    "\n",
    "# final_pred = predictions.loc[:,'Final_pred']\n",
    "\n",
    "# submit_df = pd.DataFrame({'PassengerId': pd.Series(dtype='str'),\n",
    "#                    'Transported': pd.Series(dtype='int')})\n",
    "# submit_df.loc[:,'PassengerId'] = test_df.loc[:,'PassengerId']\n",
    "# submit_df.loc[:,'Transported'] = predictions.loc[:,'Final_pred']\n",
    "# submit_df.loc[:,'Transported'] = submit_df.loc[:,'Transported'].astype('boolean')\n",
    "\n",
    "# submit_df\n",
    "# submit_df.to_csv('titanic_spaceship_submit24.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
